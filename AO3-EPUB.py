import os
import re
import json
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# è¨­å®š
cookie_txt_file = os.path.join(os.path.dirname(__file__), "") # cookie æª”æ¡ˆåç¨±æ ¹æ“šå¯¦éš›æƒ…æ³å¡«å¯«
input_file = os.path.join(os.path.dirname(__file__), "æŠ“å–æ¸…å–®.xlsx")
output_dir = os.path.join(os.path.dirname(__file__), "æŠ“å–çµæœ")
os.makedirs(output_dir, exist_ok=True)

# Cookie è®€å–
def load_cookies_from_txt(txt_path):
    cookies = {}
    with open(txt_path, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line:
                name, value = line.strip().split("=", 1)
                cookies[name] = value
    return cookies

cookies = load_cookies_from_txt(cookie_txt_file)

# å·¥å…·å‡½å¼
def clean_title(title):
    replacements = {
        ":": "ï¼š", "?": "ï¼Ÿ", "/": "ï¼", "\\": "ï¼¼",
        "*": "ï¼Š", "<": "ï¼œ", ">": "ï¼", "|": "ï½œ",
        "(": "ï¼ˆ", ")": "ï¼‰", "~": "ï½"
    }
    for k, v in replacements.items():
        title = title.replace(k, v)
    title = re.sub(r'[\n\r\t]+', '', title).strip()
    return title[:100] or "untitled"

def log_error(url, reason):
    with open("éŒ¯èª¤æ¸…å–®.txt", "a", encoding="utf-8") as f:
        f.write(f"{url}  # {reason}\n")
    print(f"âŒ éŒ¯èª¤ï¼š{url}  # {reason}")

# æŠ“å–ç¶²å€
df = pd.read_excel(input_file, sheet_name="EPUB")
urls = []
for _, row in df.iterrows():
    url = str(row.get("URL", "")).strip()
    ao3_id = str(row.get("ID", "")).strip()
    if url and url.lower() != "nan":
        urls.append(url)
    elif ao3_id and ao3_id.lower() != "nan":
        urls.append(f"https://archiveofourown.org/works/{ao3_id}")

# Header
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Referer": "https://archiveofourown.org/"
}

success_count = 0
fail_urls = []

# ä¸‹è¼‰å‡½å¼
def attempt_download(work_url):
    try:
        time.sleep(random.uniform(1.0, 3.0))
        r = requests.get(work_url + "?view_adult=true&view_full_work=true", headers=headers, cookies=cookies, timeout=180)
        if r.status_code != 200:
            log_error(work_url, f"ä½œå“é éŒ¯èª¤ HTTP {r.status_code}")
            return (work_url, "RETRY")

        soup = BeautifulSoup(r.text, "html.parser")
        title_tag = soup.select_one("h2.title")
        author_tag = soup.select_one("h3.byline a")
        title = clean_title(title_tag.text if title_tag else "untitled")
        author = author_tag.text.strip() if author_tag else "ä½œè€…ä¸æ˜"

        epub_link = soup.find("a", string="EPUB")
        if not epub_link:
            log_error(work_url, "æ‰¾ä¸åˆ° EPUB é€£çµ")
            return (work_url, "FAIL")

        download_url = "https://archiveofourown.org" + epub_link["href"]
        time.sleep(random.uniform(1.0, 2.0))
        res = requests.get(download_url, headers=headers, cookies=cookies, timeout=180)
        if res.status_code != 200:
            log_error(work_url, f"EPUB ä¸‹è¼‰éŒ¯èª¤ HTTP {res.status_code}")
            return (work_url, "FAIL")

        filename = f"{author}ã€ˆ{title}ã€‰.epub"
        filepath = os.path.join(output_dir, filename)
        with open(filepath, "wb") as f:
            f.write(res.content)
        print(f"âœ… å·²å„²å­˜ï¼š{filename}\nğŸ“ å„²å­˜è·¯å¾‘ï¼š{filepath}")
        return (work_url, "SUCCESS")

    except Exception as e:
        print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼ˆ{str(e)}ï¼‰å°‡ç¨å¾Œé‡è©¦ï¼š{work_url}")
        return (work_url, "RETRY")

# åŒæ­¥é€²è¡Œæœ€å¤šå…©ç¯‡
retry_queue = []
with ThreadPoolExecutor(max_workers=2) as executor:
    future_to_url = {executor.submit(attempt_download, url): url for url in urls}
    for future in as_completed(future_to_url):
        work_url, status = future.result()
        if status == "SUCCESS":
            success_count += 1
        elif status == "RETRY":
            retry_queue.append(work_url)
        else:
            fail_urls.append(work_url)

# é‡è©¦æœ€å¤š 10 æ¬¡
for work_url in retry_queue:
    for attempt in range(1, 11):
        print(f"ğŸ” é‡è©¦ ({attempt}/10)ï¼š{work_url}")
        _, result = attempt_download(work_url)
        if result == "SUCCESS":
            success_count += 1
            break
        elif attempt == 10:
            log_error(work_url, "é‡è©¦ 10 æ¬¡å¤±æ•—")
            fail_urls.append(work_url)
        else:
            time.sleep(attempt * 30)

# çµ±è¨ˆå ±å‘Š
print("\nğŸ“Š æŠ“å–å®Œæˆå ±å‘Š")
print(f"âœ… æˆåŠŸï¼š{success_count} ç¯‡ / {len(urls)} ç¯‡")
if fail_urls:
    print("âŒ å¤±æ•—æ¸…å–®ï¼š")
    for fail in fail_urls:
        print(f" - {fail}")
