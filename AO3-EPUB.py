import os
import re
import json
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import random

# è¨­å®š 
# cookie æª”æ¡ˆåç¨±æ ¹æ“šå¯¦éš›æƒ…æ³å¡«å¯«
cookie_txt_file = os.path.join(os.path.dirname(__file__), "")
input_file = os.path.join(os.path.dirname(__file__), "æŠ“å–æ¸…å–®.xlsx")
output_dir = os.path.join(os.path.dirname(__file__), "æŠ“å–çµæœ")
os.makedirs(output_dir, exist_ok=True)

# Cookie è®€å–ï¼ˆå¾ txt æª”ï¼‰ 
def load_cookies_from_txt(txt_path):
    cookies = {}
    with open(txt_path, "r", encoding="utf-8") as f:
        for line in f:
            if "=" in line:
                name, value = line.strip().split("=", 1)
                cookies[name] = value
    return cookies

cookies = load_cookies_from_txt(cookie_txt_file)

# å·¥å…·å‡½å¼ 
def clean_title(title):
    replacements = {
        ":": "ï¼š", "?": "ï¼Ÿ", "/": "ï¼", "\\": "ï¼¼",
        "*": "ï¼Š", "<": "ï¼œ", ">": "ï¼", "|": "ï½œ",
        "(": "ï¼ˆ", ")": "ï¼‰", "~": "ï½"
    }
    for k, v in replacements.items():
        title = title.replace(k, v)
    title = re.sub(r'[\n\r\t]+', '', title).strip()
    return title[:100] or "untitled"

def log_error(url, reason):
    with open("éŒ¯èª¤æ¸…å–®.txt", "a", encoding="utf-8") as f:
        f.write(f"{url}  # {reason}\n")
    print(f"âŒ éŒ¯èª¤ï¼š{url}  # {reason}")

# æŠ“å–ç¶²å€ 
df = pd.read_excel(input_file, sheet_name="EPUB")
urls = []
for _, row in df.iterrows():
    url = str(row.get("URL", "")).strip()
    ao3_id = str(row.get("ID", "")).strip()
    if url and url.lower() != "nan":
        urls.append(url)
    elif ao3_id and ao3_id.lower() != "nan":
        urls.append(f"https://archiveofourown.org/works/{ao3_id}")

# å¼·åŒ– Header 
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
    "Referer": "https://archiveofourown.org/",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "same-origin",
    "Upgrade-Insecure-Requests": "1"
}

retry_later = []

def attempt_download(work_url):
    try:
        time.sleep(random.randint(3, 7))  # æ¨¡æ“¬äººé¡è¡Œç‚ºå»¶é²
        r = requests.get(work_url + "?view_adult=true&view_full_work=true", headers=headers, cookies=cookies, timeout=180)
        if r.status_code == 525:
            return "RETRY"
        elif r.status_code != 200:
            log_error(work_url, f"ä½œå“é éŒ¯èª¤ HTTP {r.status_code}")
            return "FAIL"

        soup = BeautifulSoup(r.text, "html.parser")
        title_tag = soup.select_one("h2.title")
        author_tag = soup.select_one("h3.byline a")
        title = clean_title(title_tag.text if title_tag else "untitled")
        author = author_tag.text.strip() if author_tag else "ä½œè€…ä¸æ˜"

        epub_link = soup.find("a", string="EPUB")
        if not epub_link:
            log_error(work_url, "æ‰¾ä¸åˆ° EPUB é€£çµ")
            return "FAIL"

        download_url = "https://archiveofourown.org" + epub_link["href"]
        time.sleep(random.randint(2, 6))  # å†åŠ ä¸€å±¤é»æ“Šå»¶é²
        res = requests.get(download_url, headers={**headers, "Referer": work_url}, cookies=cookies, timeout=180)
        if res.status_code == 525:
            return "RETRY"
        elif res.status_code != 200:
            log_error(work_url, f"EPUB ä¸‹è¼‰éŒ¯èª¤ HTTP {res.status_code}")
            return "FAIL"

        filename = f"{title}.epub"
        filepath = os.path.join(output_dir, filename)
        with open(filepath, "wb") as f:
            f.write(res.content)
        print(f"âœ… å·²å„²å­˜ï¼š{filename}\nğŸ“ å„²å­˜è·¯å¾‘ï¼š{filepath}")
        return "SUCCESS"

    except Exception as e:
        print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼ˆ{str(e)}ï¼‰å°‡ç¨å¾Œé‡è©¦ï¼š{work_url}")
        return "RETRY"

# ç¬¬ä¸€è¼ªå˜—è©¦ï¼šæˆåŠŸçš„ç«‹å³è™•ç†ï¼Œ525 èˆ‡ä¾‹å¤–å»¶å¾Œé‡è©¦
for idx, work_url in enumerate(urls, 1):
    print(f"[{idx}/{len(urls)}] è®€å–ï¼š{work_url}")
    result = attempt_download(work_url)
    if result == "RETRY":
        retry_later.append(work_url)

# ç¬¬äºŒè¼ªé‡è©¦ï¼šæ¯å€‹æœ€å¤š 5 æ¬¡
for work_url in retry_later:
    for attempt in range(1, 6):
        print(f"ğŸ” é‡è©¦ ({attempt}/5)ï¼š{work_url}")
        result = attempt_download(work_url)
        if result == "SUCCESS":
            break
        elif attempt == 5:
            log_error(work_url, "é‡è©¦ 5 æ¬¡å¤±æ•—")
        else:
            time.sleep(attempt * 30)